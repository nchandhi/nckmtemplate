{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca205b03-f9f7-46c7-8205-e3209a7ddfe1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# %pip install azure.cognitiveservices.speech\n",
    "# %pip install azure-ai-inference\n",
    "# %pip install azure-search-documents\n",
    "# %pip install pymssql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e1bff8-6bc6-4c3a-af18-cf590ba2c1e6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import pprint\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44b152b-5fa3-4b55-98d6-97ae57bb758b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# Define the base path\n",
    "base_path = '/lakehouse/default/Files/data'\n",
    "\n",
    "# List of folders to be created\n",
    "folders = ['cu_output']\n",
    "\n",
    "# Create each folder\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(base_path, folder)\n",
    "    try:\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        print(f'Folder created at: {folder_path}')\n",
    "    except Exception as e:\n",
    "        print(f'Failed to create the folder {folder_path}. Error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a27690-cc0b-4160-a865-9278caacda28",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# get Azure AI credentials from keyvalut\n",
    "from trident_token_library_wrapper import PyTridentTokenLibrary as tl\n",
    "\n",
    "key_vault_name = 'kv_to-be-replaced'\n",
    "\n",
    "def get_secrets_from_kv(kv_name, secret_name):\n",
    "    access_token = mssparkutils.credentials.getToken(\"keyvault\")\n",
    "    kv_endpoint = f'https://{kv_name}.vault.azure.net/'\n",
    "    return(tl.get_secret_with_token(kv_endpoint,secret_name,access_token))\n",
    "\n",
    "endpoint = get_secrets_from_kv(key_vault_name,\"AZURE-OPENAI-CU-ENDPOINT\")\n",
    "api_key = get_secrets_from_kv(key_vault_name,\"AZURE-OPENAI-CU-KEY\")\n",
    "api_version = get_secrets_from_kv(key_vault_name,\"AZURE-OPENAI-CU-VERSION\")\n",
    "\n",
    "endpoint_b = spark.sparkContext.broadcast(endpoint)\n",
    "api_key_b = spark.sparkContext.broadcast(api_key)\n",
    "api_version_b = spark.sparkContext.broadcast(api_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa0b6a-2fac-4eae-a32f-4e24748382a0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# helper method to poll for inferencing results\n",
    "def poll_for_results(operation_location: str, success_state: str, failed_state: str, timeout: int = 300, interval: int = 2):\n",
    "    \"\"\"\n",
    "    Polls the operation location URL until the operation reaches a success or failure state.\n",
    "\n",
    "    Args:\n",
    "        operation_location (str): The URL to poll for the operation result.\n",
    "        success_state (str): The status indicating the operation succeeded.\n",
    "        failed_state (str): The status indicating the operation failed.\n",
    "        timeout (int, optional): Maximum time to wait in seconds. Default is 60 seconds.\n",
    "        interval (int, optional): Time between polling attempts in seconds. Default is 2 seconds.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: The final JSON response if successful, None otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    API_KEY = api_key_b.value\n",
    "    \n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': API_KEY,\n",
    "        'cogsvc-videoanalysis-face-identification-enable': \"true\"\n",
    "    }\n",
    "\n",
    "    # print(f'GET {operation_location}')\n",
    "\n",
    "    elapsed_time = 0\n",
    "    while elapsed_time <= timeout:\n",
    "        try:\n",
    "            response = requests.get(operation_location, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            # print(response)\n",
    "            # print(result)\n",
    "\n",
    "            status = result.get('status')\n",
    "            if status == success_state:\n",
    "                return result\n",
    "            elif status == failed_state:\n",
    "                print(f\"Operation failed with status: {status}\")\n",
    "                return None\n",
    "\n",
    "            time.sleep(interval)\n",
    "            elapsed_time += interval\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "    print(\"Operation timed out.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd95da0f-0b8c-49fb-ac5d-cfec63597a91",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def process_audio_file(file_path):\n",
    "    analyzer_id = 'ckm-analyzer'\n",
    "\n",
    "    # # Set content understanding service settings\n",
    "    AISERVICE_ENDPOINT = endpoint_b.value\n",
    "    API_KEY = api_key_b.value\n",
    "    API_VERSION = api_version_b.value\n",
    "\n",
    "    headers = {\n",
    "            'Ocp-Apim-Subscription-Key': API_KEY,\n",
    "            'Content-Type': 'application/octet-stream',\n",
    "            'cogsvc-videoanalysis-face-identification-enable': \"true\"\n",
    "        }\n",
    "\n",
    "    ## Set Content Understanding inference paths\n",
    "    PATH_ANALYZER_INFERENCE = \"/contentunderstanding/analyzers/{analyzerId}:analyze\"\n",
    "\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    url = f\"{AISERVICE_ENDPOINT}{PATH_ANALYZER_INFERENCE.format(analyzerId=analyzer_id)}{API_VERSION}\"\n",
    "    conversation_id = file_path.split('convo_', 1)[1].split('_')[0]\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=data)\n",
    "        \n",
    "        # Get the operation location from the response headers\n",
    "        operation_location = response.headers.get('Operation-Location')\n",
    "        if not operation_location:\n",
    "            print(\"Error: 'Operation-Location' not found in the response headers.\")\n",
    "            # return None\n",
    "\n",
    "        # Poll for results\n",
    "        result = poll_for_results(operation_location, 'Succeeded', 'Failed')\n",
    "        # print(result)\n",
    "\n",
    "        # file_name = file_path\n",
    "        start_time = file_path.replace(\".wav\", \"\")[-19:]\n",
    "        timestamp_format = \"%Y-%m-%d %H_%M_%S\"  # Adjust format if necessary\n",
    "        start_timestamp = datetime.strptime(start_time, timestamp_format)\n",
    "        start_date = start_timestamp.strftime(\"%Y-%m-%d\")\n",
    "        conversation_id = file_path.split('convo_', 1)[1].split('_')[0]\n",
    "        duration = int(result['result']['contents'][0]['fields']['Duration']['valueString'])\n",
    "        end_timestamp = str(start_timestamp + timedelta(seconds=duration))\n",
    "        end_timestamp = end_timestamp.split(\".\")[0]\n",
    "\n",
    "        conversationRow = {\n",
    "            \"ConversationId\": conversation_id,\n",
    "            \"ConversationDate\": start_date,\n",
    "            \"StartTime\": str(start_timestamp),\n",
    "            \"EndTime\": str(end_timestamp),\n",
    "            \"Duration\": duration,\n",
    "            \"Content\": result['result']['contents'][0]['fields']['content']['valueString'],\n",
    "            \"summary\": result['result']['contents'][0]['fields']['summary']['valueString'],\n",
    "            \"satisfied\": result['result']['contents'][0]['fields']['satisfied']['valueString'],\n",
    "            \"sentiment\": result['result']['contents'][0]['fields']['sentiment']['valueString'],\n",
    "            \"topic\": result['result']['contents'][0]['fields']['topic']['valueString'],\n",
    "            \"keyPhrases\": result['result']['contents'][0]['fields']['keyPhrases']['valueString'],\n",
    "            \"complaint\": result['result']['contents'][0]['fields']['complaint']['valueString']\n",
    "        }\n",
    "    except:\n",
    "        conversationRow = {\n",
    "            \"ConversationId\": conversation_id,\n",
    "            \"ConversationDate\": start_date,\n",
    "            \"StartTime\": str(start_timestamp),\n",
    "            \"EndTime\": str(start_timestamp),\n",
    "            \"Duration\": 0,\n",
    "            \"Content\": '',\n",
    "            \"summary\": '',\n",
    "            \"satisfied\": '',\n",
    "            \"sentiment\": '',\n",
    "            \"topic\": '',\n",
    "            \"keyPhrases\": '',\n",
    "            \"complaint\": ''\n",
    "        }\n",
    "\n",
    "    return conversationRow\n",
    "# test_file = '/lakehouse/default/Files/cu_audio_files_all/convo_05be369b-0a5d-4b6a-b7af-3aef1ba4e6e6_2024-12-08 22_00_00.wav'\n",
    "# process_audio_file(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975c74a7-060b-4020-97fa-6087cc44cff4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "folder_path = \"/lakehouse/default/Files/cu_audio_files_all\"\n",
    "wav_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.wav')]\n",
    "# wav_files  = wav_files[:3]\n",
    "\n",
    "df_files = spark.createDataFrame([(value,) for value in wav_files], [\"file_path\"])\n",
    "\n",
    "schema = StructType([\n",
    "             StructField(\"ConversationId\", StringType(), True),\n",
    "             StructField(\"ConversationDate\", StringType(), True),\n",
    "             StructField(\"StartTime\", StringType(), True),\n",
    "             StructField(\"EndTime\", StringType(), True),\n",
    "             StructField(\"Duration\", StringType(), True),\n",
    "             StructField(\"Content\", StringType(), True),\n",
    "             StructField(\"summary\", StringType(), True),\n",
    "             StructField(\"satisfied\", StringType(), True),\n",
    "             StructField(\"sentiment\", StringType(), True),\n",
    "             StructField(\"topic\", StringType(), True),\n",
    "             StructField(\"keyPhrases\", StringType(), True),\n",
    "             StructField(\"complaint\", StringType(), True)\n",
    "         ])\n",
    "\n",
    "process_audio_udf = udf(lambda file_path: process_audio_file(file_path),returnType=schema)\n",
    "\n",
    "df_processed = df_files.select([\"file_path\"]) \\\n",
    "                .withColumn(\"Details\", process_audio_udf(col(\"file_path\"))) \\\n",
    "                .select([ col(\"Details.ConversationId\").alias(\"ConversationId\"), \\\n",
    "                          col(\"Details.ConversationDate\").alias(\"ConversationDate\"), \\\n",
    "                          col(\"Details.StartTime\").alias(\"StartTime\"), \\\n",
    "                          col(\"Details.EndTime\").alias(\"EndTime\"), \\\n",
    "                          col(\"Details.Duration\").alias(\"Duration\"), \\\n",
    "                          col(\"Details.Content\").alias(\"Content\"), \\\n",
    "                          col(\"Details.summary\").alias(\"summary\"), \\\n",
    "                          col(\"Details.satisfied\").alias(\"satisfied\"), \\\n",
    "                          col(\"Details.sentiment\").alias(\"sentiment\"), \\\n",
    "                          col(\"Details.topic\").alias(\"topic\"), \\\n",
    "                          col(\"Details.keyPhrases\").alias(\"keyPhrases\"), \\\n",
    "                          col(\"Details.complaint\").alias(\"complaint\")\n",
    "                          ])\n",
    "\n",
    "# to adjust the dates to current date\n",
    "df_processed_pd = df_processed.toPandas()\n",
    "df_processed_pd['StartTime'] = pd.to_datetime(df_processed_pd['StartTime'], errors='coerce')\n",
    "df_processed_pd['EndTime'] = pd.to_datetime(df_processed_pd['EndTime'], errors='coerce')\n",
    "df_processed_pd['ConversationDate'] = pd.to_datetime(df_processed_pd['ConversationDate'], errors='coerce')\n",
    "\n",
    "max_start_time = df_processed_pd['StartTime'].max()\n",
    "days_difference = (datetime.today() - max_start_time).days - 1\n",
    "\n",
    "df_processed_pd['StartTime'] += pd.DateOffset(days=days_difference)\n",
    "df_processed_pd['EndTime'] += pd.DateOffset(days=days_difference)\n",
    "df_processed_pd['ConversationDate'] = (df_processed_pd['ConversationDate'] + pd.DateOffset(days=days_difference)).dt.date\n",
    "df_processed = spark.createDataFrame(df_processed_pd)\n",
    "\n",
    "# df_processed.write.format('delta').mode('append').saveAsTable('km_processed_data_cu1')                      \n",
    "df_processed.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable('km_processed_data_cu1')                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d83fc-a924-4bbf-9860-777892bae061",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import openai\n",
    "import json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-07-01-preview\" \n",
    "openai.api_base = get_secrets_from_kv(key_vault_name,\"AZURE-OPENAI-ENDPOINT\")\n",
    "openai.api_key = get_secrets_from_kv(key_vault_name,\"AZURE-OPENAI-KEY\")\n",
    "\n",
    "# sql_stmt = 'SELECT distinct topic FROM processed_data_4o_mini'\n",
    "df_data = spark.sql('''select distinct topic from km_processed_data_cu1 where content != \"\" ''')\n",
    "df = df_data.toPandas()\n",
    "\n",
    "topics_str = ', '.join(df['topic'].tolist())\n",
    "# print(topics_str)\n",
    "\n",
    "# 2. Choose the right number of topics based on data. Try to keep it as low number of topics as possible.\n",
    "\n",
    "def call_gpt4(topics_str1):\n",
    "    topic_prompt = f\"\"\"\n",
    "        You are a data analysis assistant specialized in natural language processing and topic modeling. \n",
    "        Your task is to analyze the given text corpus and identify distinct topics present within the data.\n",
    "        {topics_str1}\n",
    "        1. Identify the key topics in the text using topic modeling techniques. \n",
    "        2. Choose the right number of topics based on data. Try to keep it up to 8 topics.\n",
    "        3. Assign a clear and concise label to each topic based on its content.\n",
    "        4. Provide a brief description of each topic along with its label.\n",
    "        5. Add parental controls, billing issues like topics to the list of topics if the data includes calls related to them.\n",
    "        \n",
    "        If the input data is insufficient for reliable topic modeling, indicate that more data is needed rather than making assumptions. \n",
    "        Ensure that the topics and labels are accurate, relevant, and easy to understand.\n",
    "\n",
    "        Return the topics and their labels in JSON format.Always add 'topics' node and 'label', 'description' attriubtes in json.\n",
    "        Do not return anything else.\n",
    "        \"\"\"\n",
    "    system_prompt = 'You are a helpful assistant.'\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=\"gpt-4o-mini\", # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": topic_prompt}\n",
    "        ],\n",
    "        temperature = 0,\n",
    "        max_tokens = 2000,\n",
    "        seed=42\n",
    "    )\n",
    "    res = response['choices'][0]['message']['content']\n",
    "    return(json.loads(res.replace(\"```json\",'').replace(\"```\",'')))\n",
    "\n",
    "# Function to count the number of tokens in a string using tiktoken\n",
    "def count_tokens(text, encoding='gpt-4'):\n",
    "    tokenizer = tiktoken.encoding_for_model(encoding)\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# Function to split a comma-separated string into chunks that fit within max_tokens\n",
    "def split_data_into_chunks(text, max_tokens=2000, encoding='gpt-4'):\n",
    "    # print(\"\\n Split data input:\", text)\n",
    "    tokenizer = tiktoken.encoding_for_model(encoding)\n",
    "    # Split the string by commas\n",
    "    items = text.split(',')\n",
    "    current_chunk = []\n",
    "    all_chunks = []\n",
    "    current_token_count = 0\n",
    "\n",
    "    for item in items:\n",
    "        item = item.strip()  # Clean up any extra whitespace\n",
    "        # Count the tokens for the current item\n",
    "        item_token_count = len(tokenizer.encode(item))\n",
    "        \n",
    "        # Check if adding the item exceeds the max token limit\n",
    "        if current_token_count + item_token_count > max_tokens:\n",
    "            # Save the current chunk and start a new one\n",
    "            all_chunks.append(', '.join(current_chunk))\n",
    "            current_chunk = [item]\n",
    "            current_token_count = item_token_count\n",
    "        else:\n",
    "            # Add item to the current chunk\n",
    "            current_chunk.append(item)\n",
    "            current_token_count += item_token_count\n",
    "\n",
    "    # Append the last chunk if it has any content\n",
    "    if current_chunk:\n",
    "        all_chunks.append(', '.join(current_chunk))\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "# Define the max tokens per chunk (4096 for GPT-4)\n",
    "max_tokens = 3096\n",
    "\n",
    "# Split the string into chunks\n",
    "chunks = split_data_into_chunks(topics_str, max_tokens)\n",
    "\n",
    "def reduce_data_until_fits(topics_str, max_tokens):\n",
    "    if len(topics_str) <= max_tokens:\n",
    "        return call_gpt4(topics_str)\n",
    "    chunks = split_data_into_chunks(topics_str)\n",
    "    reduced_data = []\n",
    "\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        print(f\"Processing chunk {idx + 1}/{len(chunks)}...\")\n",
    "        try:\n",
    "            result = call_gpt4(chunk)\n",
    "            topics_object = res #json.loads(res)\n",
    "            for object1 in topics_object['topics']:\n",
    "                reduced_data.extend([object1['label']])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {idx + 1}: {str(e)}\")\n",
    "    combined_data = \", \".join(reduced_data)\n",
    "    return reduce_data_until_fits(combined_data, max_tokens)\n",
    "\n",
    "# res = reduce_data_until_fits(topics_str, max_tokens)\n",
    "topics_object = call_gpt4(topics_str)\n",
    "# res = json.loads(res.replace(\"```json\",'').replace(\"```\",''))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('label', StringType(), True), \n",
    "    StructField('description', StringType(), True)\n",
    "])\n",
    "df = spark.createDataFrame(topics_object['topics'], schema)\n",
    "df.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable('mined_topcis_cu')\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8c2394-7503-46ba-82aa-2edcc9104c04",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def get_mined_topic_mapping(input_text, list_of_topics):\n",
    "    # Construct the prompt  \n",
    "    prompt = f'''You are a data analysis assistant to help find the closest topic for a given text {input_text} \n",
    "                from a list of topics - {list_of_topics}.\n",
    "                ALLWAYS only return a topic from list - {list_of_topics}. Do not add any other text.'''\n",
    "    system_prompt = 'You are a helpful assistant.'\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            engine=\"gpt-4o-mini\", # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature = 0,\n",
    "            max_tokens = 2000\n",
    "        )\n",
    "    except:\n",
    "        time.sleep(50)\n",
    "        response = openai.ChatCompletion.create(\n",
    "            engine=\"gpt-4o-mini\", # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature = 0,\n",
    "            max_tokens = 2000\n",
    "        )\n",
    "    return response['choices'][0]['message']['content']\n",
    "    # return(json.loads(res.replace(\"```json\",'').replace(\"```\",'')))\n",
    "\n",
    "df_topics = spark.sql('select * from mined_topcis_cu').toPandas()\n",
    "mined_topics_list = df_topics['label'].tolist()\n",
    "mined_topics =  \", \".join(mined_topics_list)\n",
    "\n",
    "sql_stmt = \"select * from km_processed_data_cu1 where content != ''\"\n",
    "df_processed_data = spark.sql(sql_stmt).toPandas()\n",
    "df_processed_data['mined_topic'] = df_processed_data['topic'].apply(lambda x: get_mined_topic_mapping(x,str(mined_topics_list)))\n",
    "\n",
    "# sql_stmt = \"select * from km_processed_data_cu1 where content != '' \"\n",
    "# df_processed_data = spark.sql(sql_stmt).toPandas()\n",
    "# counter = 0\n",
    "# # call get_mined_topic_mapping function for each row in the dataframe and update the mined_topic column in the database table\n",
    "# for index, row in df_processed_data.iterrows():\n",
    "#     mined_topic_str = get_mined_topic_mapping(row['topic'], str(mined_topics_list))\n",
    "#     # update the dataframe\n",
    "#     df_processed_data.at[index, 'mined_topic'] = mined_topic_str\n",
    "\n",
    "df = spark.createDataFrame(df_processed_data)\n",
    "df.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable('km_processed_data_cu2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31767be-b2c4-42a0-992d-eb6d56138ed9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# move to after data padding\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "sql_stmt = '''select ConversationId,keyphrases,sentiment, mined_topic as topic, StartTime from km_processed_data_cu2'''\n",
    "df = spark.sql(sql_stmt)\n",
    "\n",
    "df_keyPhrases = df.withColumn('keyPhrases', split(df['keyPhrases'], ','))\n",
    "df_keyPhrases = df_keyPhrases.withColumn('keyPhrase', explode(df_keyPhrases['keyPhrases']))\n",
    "df_keyPhrases = df_keyPhrases.select('ConversationId', 'keyPhrase', 'sentiment','topic', 'StartTime')\n",
    "df_keyPhrases.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable('km_processed_data_keyphrases_cu2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201df88d-a8a0-4231-95e1-821facab322d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import pymssql\n",
    "server = get_secrets_from_kv(key_vault_name,\"SQLDB-SERVER\")\n",
    "database = get_secrets_from_kv(key_vault_name,\"SQLDB-DATABASE\")\n",
    "username = get_secrets_from_kv(key_vault_name,\"SQLDB-USERNAME\")\n",
    "password = get_secrets_from_kv(key_vault_name,\"SQLDB-PASSWORD\")\n",
    "\n",
    "conn = pymssql.connect(server, username, password, database)\n",
    "cursor = conn.cursor()\n",
    "print(\"Connected to the database\")\n",
    "\n",
    "# sql_stmt = 'SELECT distinct topic FROM processed_data'\n",
    "# cursor.execute(sql_stmt)\n",
    "\n",
    "# rows = cursor.fetchall()\n",
    "# column_names = [i[0] for i in cursor.description]\n",
    "# df = pd.DataFrame(rows, columns=column_names)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f8fdce-5640-4808-b2a5-99169c3816d1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# cursor.execute('DROP TABLE IF EXISTS processed_data')\n",
    "# conn.commit()\n",
    "\n",
    "# create_processed_data_sql = \"\"\"CREATE TABLE processed_data (\n",
    "#                 ConversationId varchar(255) NOT NULL PRIMARY KEY,\n",
    "#                 StartTime varchar(255),\n",
    "#                 EndTime varchar(255),\n",
    "#                 Content varchar(max),\n",
    "#                 summary varchar(3000),\n",
    "#                 satisfied varchar(255),\n",
    "#                 sentiment varchar(255),\n",
    "#                 topic varchar(255),\n",
    "#                 key_phrases nvarchar(max),\n",
    "#                 complaint varchar(255), \n",
    "#                 mined_topic varchar(255)\n",
    "#             );\"\"\"\n",
    "# cursor.execute(create_processed_data_sql)\n",
    "# conn.commit()\n",
    "\n",
    "\n",
    "sql_stmt = '''select ConversationId, StartTime, EndTime, Content, summary, satisfied, sentiment,\n",
    " keyphrases as key_phrases, complaint, topic, mined_topic from km_processed_data_cu2'''\n",
    "df_cu2_processed = spark.sql(sql_stmt).toPandas()\n",
    "# cursor.execute(sql_stmt)\n",
    "\n",
    "# rows = cursor.fetchall()\n",
    "# column_names = [i[0] for i in cursor.description]\n",
    "# df = pd.DataFrame(rows, columns=column_names)\n",
    "# df.rename(columns={'mined_topic': 'topic'}, inplace=True)\n",
    "# print(df.columns)\n",
    "for idx, row in df_cu2_processed.iterrows():\n",
    "    # row['ConversationId'] = str(uuid.uuid4())\n",
    "    cursor.execute(f\"INSERT INTO processed_data (ConversationId, StartTime, EndTime, Content, summary, satisfied, sentiment, topic, key_phrases, complaint, mined_topic) VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\", (row['ConversationId'], row['StartTime'], row['EndTime'], row['Content'], row['summary'], row['satisfied'], row['sentiment'], row['topic'], row['key_phrases'], row['complaint'], row['mined_topic']))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2562635",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversationIds_tuple = tuple(df_cu2_processed['ConversationId'].tolist())\n",
    "\n",
    "conversationId_vals = ', '.join(f\"'{v}'\" for v in conversationIds_tuple)\n",
    "# print(conversationId_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d186a352-a175-4642-9f30-ababc5a4a660",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import random\n",
    "sql_stmt = f\"SELECT * FROM processed_data WHERE ConversationId IN ({conversationId_vals})\"\n",
    "cursor.execute(sql_stmt)\n",
    "\n",
    "rows = cursor.fetchall()\n",
    "column_names = [i[0] for i in cursor.description]\n",
    "df = pd.DataFrame(rows, columns=column_names)\n",
    "columns_lst = df.columns\n",
    "df_append = pd.DataFrame(df, columns=columns_lst)\n",
    "days_list = [7, 14, 21, 28, 35, 42]\n",
    "rows = [5, 7, 8]\n",
    "\n",
    "# Define the sentiment values and their probabilities\n",
    "sentiment_values = ['Negative', 'Positive', 'Neutral']\n",
    "probabilities = [0.7, 0.2, 0.1]  # 70% for value1 and 30% for value2\n",
    "\n",
    "text = 'billing'\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    for i in range(random.choice(rows)):\n",
    "    \n",
    "        days = random.choice(days_list)\n",
    "       \n",
    "        if ('billing' in row['mined_topic'].lstrip().lower()) or ('issue' in row['mined_topic'].lstrip().lower()):\n",
    "            \n",
    "            row['sentiment'] = random.choices(sentiment_values, probabilities)[0] #'Negative'\n",
    "            row['satisfied'] = 'No'\n",
    "            row['EndTime'] = pd.to_datetime(row['EndTime']) - pd.to_timedelta(f\"{days} days\")\n",
    "            row['StartTime'] = pd.to_datetime(row['StartTime']) - pd.to_timedelta(f\"{days} days\")\n",
    "            row['EndTime'] = row['EndTime'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            row['StartTime'] = row['StartTime'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            row['ConversationId'] = str(uuid.uuid4())\n",
    "                \n",
    "        else:\n",
    "            row['ConversationId'] = str(uuid.uuid4())\n",
    "            row['EndTime'] = pd.to_datetime(row['EndTime']) - pd.to_timedelta(f\"{days} days\")\n",
    "            row['StartTime'] = pd.to_datetime(row['StartTime']) - pd.to_timedelta(f\"{days} days\")\n",
    "            row['EndTime'] = row['EndTime'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            row['StartTime'] = row['StartTime'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "    # write the new row to the processed_data table\n",
    "        cursor.execute(f\"INSERT INTO processed_data (ConversationId, EndTime, StartTime, Content, summary, satisfied, sentiment, topic, key_phrases, complaint, mined_topic) VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\", (row['ConversationId'], row['EndTime'], row['StartTime'], row['Content'], row['summary'], row['satisfied'], row['sentiment'], row['topic'], row['key_phrases'], row['complaint'], row['mined_topic']))\n",
    "        # add to search index \n",
    "        \n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea7b07-2cf1-4757-b4c3-01ca9d8d3248",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# cursor.execute('DROP TABLE IF EXISTS km_processed_data')\n",
    "# conn.commit()\n",
    "\n",
    "# create_processed_data_sql = \"\"\"CREATE TABLE km_processed_data (\n",
    "#                 ConversationId varchar(255) NOT NULL PRIMARY KEY,\n",
    "#                 StartTime varchar(255),\n",
    "#                 EndTime varchar(255),\n",
    "#                 Content varchar(max),\n",
    "#                 summary varchar(max),\n",
    "#                 satisfied varchar(255),\n",
    "#                 sentiment varchar(255),\n",
    "#                 keyphrases nvarchar(max),\n",
    "#                 complaint varchar(255), \n",
    "#                 topic varchar(255)\n",
    "#             );\"\"\"\n",
    "# cursor.execute(create_processed_data_sql)\n",
    "# conn.commit()\n",
    "# sql_stmt = 'SELECT * FROM processed_data'\n",
    "sql_stmt = f'''select ConversationId, StartTime, EndTime, Content, summary, satisfied, sentiment, \n",
    "key_phrases as keyphrases, complaint, mined_topic as topic from processed_data WHERE ConversationId IN ({conversationId_vals})'''\n",
    "\n",
    "cursor.execute(sql_stmt)\n",
    "\n",
    "rows = cursor.fetchall()\n",
    "column_names = [i[0] for i in cursor.description]\n",
    "df = pd.DataFrame(rows, columns=column_names)\n",
    "# df.rename(columns={'mined_topic': 'topic'}, inplace=True)\n",
    "# print(df.columns)\n",
    "for idx, row in df.iterrows():\n",
    "    # row['ConversationId'] = str(uuid.uuid4())\n",
    "    cursor.execute(f\"INSERT INTO km_processed_data (ConversationId, StartTime, EndTime, Content, summary, satisfied, sentiment, keyphrases, complaint, topic) VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\", (row['ConversationId'], row['StartTime'], row['EndTime'], row['Content'], row['summary'], row['satisfied'], row['sentiment'], row['keyphrases'], row['complaint'], row['topic']))\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd5ea08-f5cb-4aaa-bfcf-b6a9a31dfa5e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "sql_stmt = f'SELECT ConversationId,key_Phrases,sentiment, mined_topic as topic, StartTime FROM processed_data WHERE ConversationId IN ({conversationId_vals})'\n",
    "cursor.execute(sql_stmt)\n",
    "rows = cursor.fetchall()\n",
    "column_names = [i[0] for i in cursor.description]\n",
    "# df = pd.DataFrame(rows, columns=column_names)\n",
    "df = spark.createDataFrame(pd.DataFrame(rows, columns=column_names))\n",
    "df_keyPhrases = df.withColumn('keyPhrases', split(df['key_Phrases'], ','))\n",
    "df_keyPhrases = df_keyPhrases.withColumn('keyPhrase', explode(df_keyPhrases['keyPhrases']))\n",
    "df_keyPhrases = df_keyPhrases.select('ConversationId', 'keyPhrase', 'sentiment','topic', 'StartTime')\n",
    "df_keyPhrases.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable('km_processed_data_keyphrases_cu2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823fa2e6-4e2f-4580-9620-0e8a745eaa60",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# cursor.execute('DROP TABLE IF EXISTS processed_data_key_phrases')\n",
    "# conn.commit()\n",
    "\n",
    "# create_processed_data_sql = \"\"\"CREATE TABLE processed_data_key_phrases (\n",
    "#                 ConversationId varchar(255),\n",
    "#                 key_phrase varchar(500), \n",
    "#                 sentiment varchar(255),\n",
    "#                 topic varchar(255)\n",
    "#             );\"\"\"\n",
    "# cursor.execute(create_processed_data_sql)\n",
    "# conn.commit()\n",
    "\n",
    "sql_stmt = f'''select ConversationId, lower(trim(keyPhrase)) as keyPhrase,\n",
    "                 sentiment, topic, StartTime from km_processed_data_keyphrases_cu2 WHERE ConversationId IN ({conversationId_vals})'''\n",
    "df = spark.sql(sql_stmt).toPandas()\n",
    "rows = list(df.itertuples(index=False, name=None))\n",
    "# print(rows)\n",
    "\n",
    "# Generate the SQL query for insertion\n",
    "insert_query = f\"INSERT INTO processed_data_key_phrases (ConversationId, key_phrase, sentiment,topic, StartTime) VALUES (%s, %s, %s, %s, %s)\"\n",
    "\n",
    "# # Perform the bulk insert\n",
    "# cursor.executemany(insert_query, rows)\n",
    "\n",
    "chunk_size = 1000\n",
    "for i in range(0, len(rows), chunk_size):\n",
    "    cursor.executemany(insert_query, rows[i:i + chunk_size])\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827ceb02-6165-4fdb-8c17-3052f43b3e46",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# cursor.execute('DROP TABLE IF EXISTS processed_data_key_phrases')\n",
    "# conn.commit()\n",
    "\n",
    "# create_processed_data_sql = \"\"\"CREATE TABLE processed_data_key_phrases (\n",
    "#                 ConversationId varchar(255),\n",
    "#                 key_phrase varchar(500), \n",
    "#                 sentiment varchar(255),\n",
    "#                 topic varchar(255)\n",
    "#             );\"\"\"\n",
    "# cursor.execute(create_processed_data_sql)\n",
    "# conn.commit()\n",
    "\n",
    "# # sql_stmt = 'SELECT ConversationId,key_Phrases,sentiment, mined_topic as topic FROM processed_data'\n",
    "# # cursor.execute(sql_stmt)\n",
    "# # rows = cursor.fetchall()\n",
    "# # column_names = [i[0] for i in cursor.description]\n",
    "# # df = pd.DataFrame(rows, columns=column_names)\n",
    "# # # print(df.columns)\n",
    "# # for idx, row in df.iterrows():\n",
    "# #     key_phrases = row['key_Phrases'].split(',')\n",
    "# #     for key_phrase in key_phrases:\n",
    "# #         key_phrase = key_phrase.strip()\n",
    "# #         cursor.execute(f\"INSERT INTO processed_data_key_phrases (ConversationId, key_phrase, sentiment,topic) VALUES (%s,%s,%s,%s)\", (row['ConversationId'], key_phrase, row['sentiment'],row['topic']))\n",
    "\n",
    "# sql_stmt = '''select ConversationId, lower(trim(keyPhrase)) as keyPhrase, sentiment, topic from km_processed_data_keyphrases_cu2'''\n",
    "# df = spark.sql(sql_stmt).toPandas()\n",
    "\n",
    "# # # sql_stmt = 'SELECT * FROM processed_data'\n",
    "# # # cursor.execute(sql_stmt)\n",
    "# # # rows = cursor.fetchall()\n",
    "# # # column_names = [i[0] for i in cursor.description]\n",
    "# # # df = pd.DataFrame(rows, columns=column_names)\n",
    "# # # df.rename(columns={'mined_topic': 'topic'}, inplace=True)\n",
    "# # # print(df.columns)\n",
    "# for idx, row in df.iterrows():\n",
    "#     # row['ConversationId'] = str(uuid.uuid4())\n",
    "#     cursor.execute(f\"INSERT INTO processed_data_key_phrases (ConversationId, key_phrase, sentiment, topic) VALUES (%s,%s,%s,%s)\", (row['ConversationId'], row['keyPhrase'].strip(),row['sentiment'],row['topic']))\n",
    "# conn.commit()\n",
    "# cursor.close()\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d14da9c-5a9d-464e-a229-a04d3a648292",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# %%sql\n",
    "# select * from km_processed_data_keyphrases_cu2 order by topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c377d6-f63e-4e77-8cd3-74e581337497",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# %%sql\n",
    "# select lower(trim(keyPhrase)), sentiment,count(*) from km_processed_data_keyphrases_cu2\n",
    "# -- where trim(keyPhrase) not in ('account number','Contoso Incorporated','phone number')\n",
    "# group by lower(trim(keyPhrase)), sentiment\n",
    "# order by count(*) desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026f3369-5da8-43cc-82aa-fb59b786fded",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# %%sql\n",
    "# select mined_topic, sentiment,count(*) from km_processed_data_cu2\n",
    "# group by mined_topic, sentiment\n",
    "# order by mined_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a644b0-3724-4652-8566-926385834738",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Directory paths\n",
    "input_dir = '/lakehouse/default/Files/cu_audio_files_all/'\n",
    "processed_dir = '/lakehouse/default/Files/cu_audio_files_processed/'\n",
    "# failed_folder = '/lakehouse/default/Files/data/conversation_failed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb263e-8810-4b59-a454-5541e8b4a509",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# # This cell creates new folders within the specified base path in the lakehouse. \n",
    "# The purpose is to create corresponding folders so files can be moved as they are processed.\n",
    "\n",
    "import os \n",
    "\n",
    "# Define the base path\n",
    "base_path = '/lakehouse/default/Files/'\n",
    "\n",
    "# List of folders to be created\n",
    "folders = ['cu_audio_files_processed']\n",
    "\n",
    "# Create each folder\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(base_path, folder)\n",
    "    try:\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        print(f'Folder created at: {folder_path}')\n",
    "    except Exception as e:\n",
    "        print(f'Failed to create the folder {folder_path}. Error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e7518-5f77-4f47-bd79-ee7b152fea2e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Move input files to processed directory\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Get a list of all .json files in the input directory\n",
    "wav_files = [f for f in os.listdir(input_dir) if f.endswith('.wav')]\n",
    "\n",
    "print(wav_files)\n",
    "\n",
    "# Move each .json file to the processed directory\n",
    "for file_name in wav_files:\n",
    "    shutil.move(os.path.join(input_dir, file_name), os.path.join(processed_dir, file_name))"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "443b2bf2-b9a6-4c82-af36-8c3b048ac807",
    "workspaceId": "fe70d7ea-406d-4f0f-8d9e-d7893c175c1e"
   },
   "lakehouse": {
    "default_lakehouse": "b8d2acab-78cf-4892-bb7b-443b9d6df9e0",
    "default_lakehouse_name": "lakehouse_bk1",
    "default_lakehouse_workspace_id": "fe70d7ea-406d-4f0f-8d9e-d7893c175c1e"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
