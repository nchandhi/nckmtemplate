{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee657b8c-825f-4195-a323-7cc1345feda6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedb96f0-8854-486b-88f3-85344b3db2ae",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import ast\n",
    "import base64\n",
    "import pandas as pd\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f2f8f4-8c1e-40cb-b05e-95c2b1715c8a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "key_vault_name = 'kv_to-be-replaced'\n",
    "table_name = 'processed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6daac19-761b-41c0-ba81-1987cd68fa8d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from trident_token_library_wrapper import PyTridentTokenLibrary as tl\n",
    "def get_secrets_from_kv(kv_name, secret_name):\n",
    "    access_token = mssparkutils.credentials.getToken(\"keyvault\")\n",
    "    kv_endpoint = f'https://{kv_name}.vault.azure.net/'\n",
    "    return(tl.get_secret_with_token(kv_endpoint,secret_name,access_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02537cde-09dc-4bdc-aea7-ec50be2675a8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# GPT-4o-mini\n",
    "# api_key = get_secrets_from_kv(key_vault_name,\"AZURE-OPENAI-KEY\")\n",
    "# api_type = \"azure\"\n",
    "# api_version = get_secrets_from_kv(key_vault_name, \"AZURE-OPENAI-PREVIEW-API-VERSION\")\n",
    "# endpoint = get_secrets_from_kv(key_vault_name,\"AZURE-OPENAI-ENDPOINT\")\n",
    "\n",
    "# Phi-3\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "endpoint= get_secrets_from_kv(key_vault_name, \"AZURE-OPENAI-INFERENCE-ENDPOINT\")\n",
    "api_key = get_secrets_from_kv(key_vault_name, \"AZURE-OPENAI-INFERENCE-KEY\")\n",
    "client = ChatCompletionsClient(endpoint=endpoint, credential=AzureKeyCredential(api_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affc65fc-95fe-4c45-95a6-2792a9e5fcf7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "sql_stmt = f'SELECT distinct topic FROM {table_name}'\n",
    "df = spark.sql(sql_stmt).toPandas()\n",
    "\n",
    "topics_str = ', '.join(df['topic'].tolist())\n",
    "\n",
    "\n",
    "def call_gpt4(topics_str1):\n",
    "    topic_prompt = f\"\"\"\n",
    "        You are a data analysis assistant specialized in natural language processing and topic modeling. \n",
    "        Your task is to analyze the given text corpus and identify distinct topics present within the data.\n",
    "        {topics_str1}\n",
    "        1. Identify the key topics in the text using topic modeling techniques. \n",
    "        2. Choose the right number of topics based on data. Try to keep it up to 8 topics.\n",
    "        3. Assign a clear and concise label to each topic based on its content.\n",
    "        4. Provide a brief description of each topic along with its label.\n",
    "        5. Add parental controls, billing issues like topics to the list of topics if the data includes calls related to them.\n",
    "        \n",
    "        If the input data is insufficient for reliable topic modeling, indicate that more data is needed rather than making assumptions. \n",
    "        Ensure that the topics and labels are accurate, relevant, and easy to understand.\n",
    "\n",
    "        Return the topics and their labels in JSON format.Always add 'topics' node and 'label', 'description' attriubtes in json.\n",
    "        Do not return anything else.\n",
    "        \"\"\"\n",
    "    # GPT-4o-mini\n",
    "    # system_prompt = 'You are a helpful assistant.'\n",
    "    # response = openai.ChatCompletion.create(\n",
    "    #     engine=\"gpt-4o-mini\", # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "    #     messages=[\n",
    "    #         {\"role\": \"system\", \"content\": system_prompt},\n",
    "    #         {\"role\": \"user\", \"content\": topic_prompt}\n",
    "    #     ],\n",
    "    #     temperature = 0,\n",
    "    #     max_tokens = 2000\n",
    "    # )\n",
    "    # res = response['choices'][0]['message']['content']\n",
    "    # return(json.loads(res.replace(\"```json\",'').replace(\"```\",'')))\n",
    "\n",
    "    # Phi-3 model    \n",
    "    response = client.complete(\n",
    "        messages=[\n",
    "            # SystemMessage(content=prompt),\n",
    "            UserMessage(content=topic_prompt),\n",
    "        ],\n",
    "        max_tokens = 500,\n",
    "        temperature = 0,\n",
    "        top_p = 1\n",
    "    )\n",
    "\n",
    "    res = response.choices[0].message.content\n",
    "    return(json.loads(res.replace(\"```json\",'').replace(\"```\",'')))\n",
    "\n",
    "# Function to count the number of tokens in a string using tiktoken\n",
    "def count_tokens(text, encoding='gpt-4'):\n",
    "    tokenizer = tiktoken.encoding_for_model(encoding)\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# Function to split a comma-separated string into chunks that fit within max_tokens\n",
    "def split_data_into_chunks(text, max_tokens=2000, encoding='gpt-4'):\n",
    "    print(\"\\n Split data input:\", text)\n",
    "    tokenizer = tiktoken.encoding_for_model(encoding)\n",
    "    # Split the string by commas\n",
    "    items = text.split(',')\n",
    "    current_chunk = []\n",
    "    all_chunks = []\n",
    "    current_token_count = 0\n",
    "\n",
    "    for item in items:\n",
    "        item = item.strip()  # Clean up any extra whitespace\n",
    "        # Count the tokens for the current item\n",
    "        item_token_count = len(tokenizer.encode(item))\n",
    "        \n",
    "        # Check if adding the item exceeds the max token limit\n",
    "        if current_token_count + item_token_count > max_tokens:\n",
    "            # Save the current chunk and start a new one\n",
    "            all_chunks.append(', '.join(current_chunk))\n",
    "            current_chunk = [item]\n",
    "            current_token_count = item_token_count\n",
    "        else:\n",
    "            # Add item to the current chunk\n",
    "            current_chunk.append(item)\n",
    "            current_token_count += item_token_count\n",
    "\n",
    "    # Append the last chunk if it has any content\n",
    "    if current_chunk:\n",
    "        all_chunks.append(', '.join(current_chunk))\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "# Define the max tokens per chunk (4096 for GPT-4)\n",
    "max_tokens = 3096\n",
    "\n",
    "# Split the string into chunks\n",
    "chunks = split_data_into_chunks(topics_str, max_tokens)\n",
    "\n",
    "def reduce_data_until_fits(topics_str, max_tokens):\n",
    "    if len(topics_str) <= max_tokens:\n",
    "        return call_gpt4(topics_str)\n",
    "    chunks = split_data_into_chunks(topics_str)\n",
    "    reduced_data = []\n",
    "\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        print(f\"Processing chunk {idx + 1}/{len(chunks)}...\")\n",
    "        try:\n",
    "            result = call_gpt4(chunk)\n",
    "            topics_object = res #json.loads(res)\n",
    "            for object1 in topics_object['topics']:\n",
    "                reduced_data.extend([object1['label']])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {idx + 1}: {str(e)}\")\n",
    "    combined_data = \", \".join(reduced_data)\n",
    "    return reduce_data_until_fits(combined_data, max_tokens)\n",
    "\n",
    "res = call_gpt4(topics_str)\n",
    "topics_object = res \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9905c425-461a-4181-a63a-d9d9fe5d03d3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('label', StringType(), True), \n",
    "    StructField('description', StringType(), True)\n",
    "])\n",
    "df = spark.createDataFrame(topics_object['topics'], schema)\n",
    "df.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable('mined_topcis')\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c32b72-405c-4ccc-be77-b40cea49cdde",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def get_mined_topic_mapping(input_text, list_of_topics):\n",
    "    # Construct the prompt  \n",
    "    prompt = f'''You are a data analysis assistant to help find topic from a given text {input_text} \n",
    "             and a list of predefined topics {list_of_topics}.  \n",
    "             Always find the topic from {list_of_topics}. Do not add new topics.\n",
    "            Only return topic and nothing else.'''\n",
    "    system_prompt = 'You are a helpful assistant.'\n",
    "    # response = openai.ChatCompletion.create(\n",
    "    #     engine=\"gpt-4o-mini\", # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "    #     messages=[\n",
    "    #         {\"role\": \"system\", \"content\": system_prompt},\n",
    "    #         {\"role\": \"user\", \"content\": prompt}\n",
    "    #     ],\n",
    "    #     temperature = 0,\n",
    "    #     max_tokens = 2000\n",
    "    # )\n",
    "    # return response['choices'][0]['message']['content']\n",
    "    # return(json.loads(res.replace(\"```json\",'').replace(\"```\",'')))\n",
    "\n",
    "    # Phi-3 model    \n",
    "    response = client.complete(\n",
    "        messages=[\n",
    "            # SystemMessage(content=prompt),\n",
    "            UserMessage(content=prompt),\n",
    "        ],\n",
    "        max_tokens = 500,\n",
    "        temperature = 0,\n",
    "        top_p = 1\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "    # return(json.loads(res.replace(\"```json\",'').replace(\"```\",'')))\n",
    "\n",
    "df_topics = spark.sql('select * from mined_topcis').toPandas()\n",
    "mined_topics_list = df_topics['label'].tolist()\n",
    "mined_topics =  \", \".join(mined_topics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4278072-6311-4f56-85ac-3e02f329d671",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "sql_stmt = 'select * from processed_data'\n",
    "df_processed_data = spark.sql(sql_stmt).toPandas()\n",
    "counter = 0\n",
    "# call get_mined_topic_mapping function for each row in the dataframe and update the mined_topic column in the database table\n",
    "for index, row in df_processed_data.iterrows():\n",
    "    mined_topic_str = get_mined_topic_mapping(row['topic'], mined_topics)\n",
    "    # update the dataframe\n",
    "    df_processed_data.at[index, 'mined_topic'] = mined_topic_str\n",
    "    # print(mined_topic_str)\n",
    "    # break\n",
    "df = spark.createDataFrame(df_processed_data)\n",
    "df.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable('processed_data')"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "510c3aa4-bff3-4074-af2d-27d65569dd4f",
    "default_lakehouse_name": "KMLakeHouse",
    "default_lakehouse_workspace_id": "f59a724c-3eae-438f-93bf-2e9e1a4b9edc"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
